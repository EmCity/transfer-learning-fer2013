{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (50000, 1, 64, 64)\n",
      "validation set: (7246, 1, 64, 64)\n",
      "testing set: (3700, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from constants import *\n",
    "\n",
    "# load\n",
    "train_data = np.load(SAVE_PATH + \"face_train_data.npy\")\n",
    "train_label = np.load(SAVE_PATH + \"face_train_label.npy\")\n",
    "X_test = np.load(SAVE_PATH + \"face_test_data.npy\")\n",
    "y_test = np.load(SAVE_PATH + \"face_test_label.npy\")\n",
    "\n",
    "# reshape\n",
    "train_data = np.reshape(train_data, (-1, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "X_test = np.reshape(X_test, (-1, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "# shuffle and split train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_label, test_size=0.1, shuffle=True)\n",
    "\n",
    "# adjust the size\n",
    "X_val = np.concatenate([X_train[50000:], X_val], axis=0)\n",
    "X_train = X_train[:50000]\n",
    "y_val = np.concatenate([y_train[50000:], y_val], axis=0)\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# info\n",
    "print(\"training set: {}\".format(X_train.shape))\n",
    "print(\"validation set: {}\".format(X_val.shape))\n",
    "print(\"testing set: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class EmoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmoNet, self).__init__()\n",
    "        self.extractor = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(2, 2),\n",
    "                                       nn.BatchNorm2d(32),\n",
    "                                       nn.Dropout(0.3),\n",
    "                                       nn.Conv2d(32, 64, 3, padding=1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(2, 2),\n",
    "                                       nn.BatchNorm2d(64),\n",
    "                                       nn.Dropout(0.3),\n",
    "                                       nn.Conv2d(64, 64, 3, padding=1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(2, 2),\n",
    "                                       nn.BatchNorm2d(64),\n",
    "                                       nn.Dropout(0.3),\n",
    "                                       nn.Conv2d(64, 128, 3, padding=1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(2, 2),\n",
    "                                       nn.BatchNorm2d(128))\n",
    "        self.classifier = nn.Sequential(nn.Linear(128 * 4 * 4, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(),\n",
    "                                        nn.Linear(128, 6),\n",
    "                                        nn.Sigmoid())\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = Variable(torch.from_numpy(inputs)).float().cuda()\n",
    "        feature = self.extractor(inputs)\n",
    "        flattened = feature.view(-1, feature.size(1) * feature.size(2) * feature.size(3))\n",
    "        outputs = self.classifier(flattened)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        pred = self.forward(inputs)\n",
    "        _, pred = torch.max(pred, dim=1)\n",
    "        pred = pred.cpu().data.numpy()\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def eval(self, inputs, labels):\n",
    "        pred = self.predict(inputs)\n",
    "        \n",
    "        return accuracy_score(pred, labels)\n",
    "    \n",
    "    def test(self, inputs, labels, batch):\n",
    "        num_iter = inputs.shape[0] // batch\n",
    "        inputs_list = np.array_split(inputs, num_iter)\n",
    "        labels_list = np.array_split(labels, num_iter)\n",
    "        scores = []\n",
    "        for i in range(num_iter):\n",
    "            scores.append(self.eval(inputs_list[i], labels_list[i]))\n",
    "            \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val, lr, batch, epoch):\n",
    "        num_iter_train = X_train.shape[0] // batch\n",
    "        num_iter_val = X_val.shape[0] // batch\n",
    "        X_train_list = np.array_split(X_train, num_iter_train, axis=0)\n",
    "        y_train_list = np.array_split(y_train, num_iter_train, axis=0)\n",
    "        X_val_list = np.array_split(X_val, num_iter_val, axis=0)\n",
    "        y_val_list = np.array_split(y_val, num_iter_val, axis=0)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        # reduce the lr in validation acc plateau\n",
    "        scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
    "        for epoch_id in range(epoch):\n",
    "            running_train_acc = []\n",
    "            running_val_acc = []\n",
    "            running_train_loss = 0.0\n",
    "            running_val_loss = 0.0\n",
    "            # train\n",
    "            for iter_id in range(num_iter_train):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(X_train_list[iter_id])\n",
    "                running_train_acc.append(self.eval(X_train_list[iter_id], y_train_list[iter_id]))\n",
    "                train_loss = criterion(outputs, Variable(torch.from_numpy(y_train_list[iter_id])).long().cuda())\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss += train_loss.data[0]\n",
    "            self.train_acc.append(np.mean(running_train_acc))\n",
    "            # val\n",
    "            for iter_id in range(num_iter_val):\n",
    "                running_val_acc.append(self.eval(X_val_list[iter_id], y_val_list[iter_id]))\n",
    "                running_val_loss += criterion(self.forward(X_val_list[iter_id]), Variable(torch.from_numpy(y_val_list[iter_id])).long().cuda()).data[0]\n",
    "            self.val_acc.append(np.mean(running_val_acc))\n",
    "            # lr decay\n",
    "            scheduler.step(running_val_loss / num_iter_val)\n",
    "            # info per epoch\n",
    "            print(\"epoch: {}, train_loss: {}, train_acc: {}, val_loss: {}, val_acc: {}\".format(\n",
    "                    epoch_id + 1,\n",
    "                    round(running_train_loss / num_iter_train, 5),\n",
    "                    round(self.train_acc[-1], 5),\n",
    "                    round(running_val_loss / num_iter_val, 5),\n",
    "                    round(self.val_acc[-1], 5)\n",
    "                ))\n",
    "        print(\"complete...\\n\")\n",
    "        \n",
    "    def draw_curve(self):        \n",
    "        plt.plot(range(len(self.train_acc)), self.train_acc, 'b-', label='train_acc')\n",
    "        plt.plot(range(len(self.val_acc)), self.val_acc, 'r-', label='val_acc')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmoNet(\n",
      "  (extractor): Sequential(\n",
      "    (0): Conv2d (1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): Dropout(p=0.3)\n",
      "    (5): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (9): Dropout(p=0.3)\n",
      "    (10): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (14): Dropout(p=0.3)\n",
      "    (15): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): ReLU()\n",
      "    (17): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=128)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=128, out_features=6)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = EmoNet().cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 1.61554, train_acc: 0.3818, val_loss: 1.53547, val_acc: 0.46438\n",
      "epoch: 2, train_loss: 1.50531, train_acc: 0.48512, val_loss: 1.48743, val_acc: 0.50138\n",
      "epoch: 3, train_loss: 1.46836, train_acc: 0.52064, val_loss: 1.46032, val_acc: 0.52415\n",
      "epoch: 4, train_loss: 1.44602, train_acc: 0.54308, val_loss: 1.45099, val_acc: 0.54016\n",
      "epoch: 5, train_loss: 1.42818, train_acc: 0.55898, val_loss: 1.43926, val_acc: 0.55106\n",
      "epoch: 6, train_loss: 1.41528, train_acc: 0.56902, val_loss: 1.43799, val_acc: 0.55934\n",
      "epoch: 7, train_loss: 1.40706, train_acc: 0.58058, val_loss: 1.42461, val_acc: 0.56444\n",
      "epoch: 8, train_loss: 1.39721, train_acc: 0.5875, val_loss: 1.4256, val_acc: 0.57149\n",
      "epoch: 9, train_loss: 1.38874, train_acc: 0.59616, val_loss: 1.42324, val_acc: 0.56927\n",
      "epoch: 10, train_loss: 1.38272, train_acc: 0.60496, val_loss: 1.41783, val_acc: 0.57065\n",
      "epoch: 11, train_loss: 1.37914, train_acc: 0.60778, val_loss: 1.41156, val_acc: 0.5799\n",
      "epoch: 12, train_loss: 1.37277, train_acc: 0.61204, val_loss: 1.41366, val_acc: 0.57383\n",
      "epoch: 13, train_loss: 1.36844, train_acc: 0.61508, val_loss: 1.41892, val_acc: 0.5737\n",
      "epoch: 14, train_loss: 1.36187, train_acc: 0.6247, val_loss: 1.41383, val_acc: 0.58156\n",
      "epoch: 15, train_loss: 1.36039, train_acc: 0.62872, val_loss: 1.40858, val_acc: 0.58156\n",
      "epoch: 16, train_loss: 1.35584, train_acc: 0.63024, val_loss: 1.4107, val_acc: 0.57852\n",
      "epoch: 17, train_loss: 1.35214, train_acc: 0.63228, val_loss: 1.41077, val_acc: 0.58115\n",
      "epoch: 18, train_loss: 1.34846, train_acc: 0.64194, val_loss: 1.40842, val_acc: 0.58626\n",
      "epoch: 19, train_loss: 1.34732, train_acc: 0.63984, val_loss: 1.41009, val_acc: 0.58984\n",
      "epoch: 20, train_loss: 1.34278, train_acc: 0.64334, val_loss: 1.40725, val_acc: 0.59068\n",
      "epoch: 21, train_loss: 1.33841, train_acc: 0.64692, val_loss: 1.40574, val_acc: 0.59164\n",
      "epoch: 22, train_loss: 1.33638, train_acc: 0.64912, val_loss: 1.4074, val_acc: 0.58474\n",
      "epoch: 23, train_loss: 1.33566, train_acc: 0.65096, val_loss: 1.40217, val_acc: 0.5926\n",
      "epoch: 24, train_loss: 1.332, train_acc: 0.6546, val_loss: 1.40526, val_acc: 0.58267\n",
      "epoch: 25, train_loss: 1.32856, train_acc: 0.65868, val_loss: 1.40542, val_acc: 0.59039\n",
      "epoch: 26, train_loss: 1.32812, train_acc: 0.66206, val_loss: 1.40574, val_acc: 0.58488\n",
      "epoch: 27, train_loss: 1.32841, train_acc: 0.65798, val_loss: 1.40489, val_acc: 0.59343\n",
      "epoch: 28, train_loss: 1.32519, train_acc: 0.66314, val_loss: 1.40079, val_acc: 0.58957\n",
      "epoch: 29, train_loss: 1.32536, train_acc: 0.66226, val_loss: 1.39713, val_acc: 0.58915\n",
      "epoch: 30, train_loss: 1.32275, train_acc: 0.66786, val_loss: 1.40471, val_acc: 0.58888\n",
      "epoch: 31, train_loss: 1.3192, train_acc: 0.6702, val_loss: 1.40185, val_acc: 0.59067\n",
      "epoch: 32, train_loss: 1.31694, train_acc: 0.67126, val_loss: 1.40028, val_acc: 0.59232\n",
      "epoch: 33, train_loss: 1.31608, train_acc: 0.67704, val_loss: 1.39924, val_acc: 0.59204\n",
      "epoch: 34, train_loss: 1.31526, train_acc: 0.67722, val_loss: 1.40334, val_acc: 0.59605\n",
      "epoch: 35, train_loss: 1.31306, train_acc: 0.67544, val_loss: 1.40158, val_acc: 0.59412\n",
      "epoch: 36, train_loss: 1.3039, train_acc: 0.68374, val_loss: 1.3958, val_acc: 0.59288\n",
      "epoch: 37, train_loss: 1.29998, train_acc: 0.68924, val_loss: 1.39419, val_acc: 0.60323\n",
      "epoch: 38, train_loss: 1.29924, train_acc: 0.6923, val_loss: 1.39485, val_acc: 0.59522\n",
      "epoch: 39, train_loss: 1.29548, train_acc: 0.6954, val_loss: 1.39406, val_acc: 0.60061\n",
      "epoch: 40, train_loss: 1.29393, train_acc: 0.69648, val_loss: 1.39623, val_acc: 0.59937\n",
      "epoch: 41, train_loss: 1.29343, train_acc: 0.69672, val_loss: 1.39438, val_acc: 0.59012\n",
      "epoch: 42, train_loss: 1.29232, train_acc: 0.69816, val_loss: 1.39388, val_acc: 0.6013\n",
      "epoch: 43, train_loss: 1.29289, train_acc: 0.69842, val_loss: 1.39074, val_acc: 0.59936\n",
      "epoch: 44, train_loss: 1.29146, train_acc: 0.7026, val_loss: 1.39174, val_acc: 0.60309\n",
      "epoch: 45, train_loss: 1.29063, train_acc: 0.70212, val_loss: 1.39702, val_acc: 0.59813\n",
      "epoch: 46, train_loss: 1.29083, train_acc: 0.70134, val_loss: 1.39572, val_acc: 0.59798\n",
      "epoch: 47, train_loss: 1.28797, train_acc: 0.70388, val_loss: 1.38987, val_acc: 0.59771\n",
      "epoch: 48, train_loss: 1.28752, train_acc: 0.70366, val_loss: 1.38767, val_acc: 0.61012\n",
      "epoch: 49, train_loss: 1.28576, train_acc: 0.7046, val_loss: 1.39239, val_acc: 0.60171\n",
      "epoch: 50, train_loss: 1.28696, train_acc: 0.70436, val_loss: 1.38882, val_acc: 0.60185\n",
      "epoch: 51, train_loss: 1.28588, train_acc: 0.70738, val_loss: 1.38982, val_acc: 0.59978\n",
      "epoch: 52, train_loss: 1.28423, train_acc: 0.70726, val_loss: 1.39396, val_acc: 0.60253\n",
      "epoch: 53, train_loss: 1.2838, train_acc: 0.7082, val_loss: 1.39227, val_acc: 0.60654\n",
      "epoch: 54, train_loss: 1.28337, train_acc: 0.70772, val_loss: 1.38936, val_acc: 0.60281\n",
      "epoch: 55, train_loss: 1.28264, train_acc: 0.71068, val_loss: 1.39008, val_acc: 0.59991\n",
      "epoch: 56, train_loss: 1.28249, train_acc: 0.71116, val_loss: 1.38874, val_acc: 0.59688\n",
      "epoch: 57, train_loss: 1.28207, train_acc: 0.71028, val_loss: 1.38706, val_acc: 0.60639\n",
      "epoch: 58, train_loss: 1.28151, train_acc: 0.7111, val_loss: 1.38563, val_acc: 0.60213\n",
      "epoch: 59, train_loss: 1.28256, train_acc: 0.71226, val_loss: 1.39109, val_acc: 0.59812\n",
      "epoch: 60, train_loss: 1.28215, train_acc: 0.70982, val_loss: 1.39091, val_acc: 0.60488\n",
      "epoch: 61, train_loss: 1.28001, train_acc: 0.71224, val_loss: 1.3904, val_acc: 0.60296\n",
      "epoch: 62, train_loss: 1.27937, train_acc: 0.71212, val_loss: 1.38953, val_acc: 0.60337\n",
      "epoch: 63, train_loss: 1.28268, train_acc: 0.71242, val_loss: 1.38677, val_acc: 0.60254\n",
      "epoch: 64, train_loss: 1.27999, train_acc: 0.71202, val_loss: 1.3953, val_acc: 0.60102\n",
      "epoch: 65, train_loss: 1.28033, train_acc: 0.71018, val_loss: 1.3953, val_acc: 0.60267\n",
      "epoch: 66, train_loss: 1.28173, train_acc: 0.71176, val_loss: 1.38983, val_acc: 0.59812\n",
      "epoch: 67, train_loss: 1.28058, train_acc: 0.70942, val_loss: 1.39363, val_acc: 0.60861\n",
      "epoch: 68, train_loss: 1.28079, train_acc: 0.71118, val_loss: 1.39425, val_acc: 0.60475\n",
      "epoch: 69, train_loss: 1.28055, train_acc: 0.70798, val_loss: 1.39178, val_acc: 0.60088\n",
      "epoch: 70, train_loss: 1.28041, train_acc: 0.71186, val_loss: 1.39166, val_acc: 0.60765\n",
      "epoch: 71, train_loss: 1.28208, train_acc: 0.71218, val_loss: 1.39239, val_acc: 0.60612\n",
      "epoch: 72, train_loss: 1.27968, train_acc: 0.71268, val_loss: 1.39197, val_acc: 0.6071\n",
      "epoch: 73, train_loss: 1.27923, train_acc: 0.71094, val_loss: 1.39029, val_acc: 0.60116\n",
      "epoch: 74, train_loss: 1.2809, train_acc: 0.71154, val_loss: 1.38552, val_acc: 0.6082\n",
      "epoch: 75, train_loss: 1.2788, train_acc: 0.71264, val_loss: 1.39337, val_acc: 0.60931\n",
      "epoch: 76, train_loss: 1.28115, train_acc: 0.712, val_loss: 1.39358, val_acc: 0.60668\n",
      "epoch: 77, train_loss: 1.28195, train_acc: 0.70976, val_loss: 1.3882, val_acc: 0.60379\n",
      "epoch: 78, train_loss: 1.28009, train_acc: 0.71192, val_loss: 1.38893, val_acc: 0.5995\n",
      "epoch: 79, train_loss: 1.28058, train_acc: 0.70988, val_loss: 1.39445, val_acc: 0.60213\n",
      "epoch: 80, train_loss: 1.28054, train_acc: 0.71234, val_loss: 1.39156, val_acc: 0.60406\n",
      "epoch: 81, train_loss: 1.281, train_acc: 0.71128, val_loss: 1.39009, val_acc: 0.60322\n",
      "epoch: 82, train_loss: 1.28018, train_acc: 0.70972, val_loss: 1.39167, val_acc: 0.60641\n",
      "epoch: 83, train_loss: 1.2809, train_acc: 0.7107, val_loss: 1.39015, val_acc: 0.60944\n",
      "epoch: 84, train_loss: 1.28141, train_acc: 0.71166, val_loss: 1.38679, val_acc: 0.60254\n",
      "epoch: 85, train_loss: 1.27959, train_acc: 0.7114, val_loss: 1.39082, val_acc: 0.60254\n",
      "epoch: 86, train_loss: 1.28051, train_acc: 0.71168, val_loss: 1.39245, val_acc: 0.60488\n",
      "epoch: 87, train_loss: 1.27982, train_acc: 0.71282, val_loss: 1.39145, val_acc: 0.60778\n",
      "epoch: 88, train_loss: 1.28143, train_acc: 0.7137, val_loss: 1.39639, val_acc: 0.60447\n",
      "epoch: 89, train_loss: 1.27886, train_acc: 0.71184, val_loss: 1.39267, val_acc: 0.60213\n",
      "epoch: 90, train_loss: 1.28086, train_acc: 0.70972, val_loss: 1.39466, val_acc: 0.60516\n",
      "epoch: 91, train_loss: 1.27989, train_acc: 0.71406, val_loss: 1.3875, val_acc: 0.59922\n",
      "epoch: 92, train_loss: 1.28098, train_acc: 0.71016, val_loss: 1.39259, val_acc: 0.60157\n",
      "epoch: 93, train_loss: 1.28007, train_acc: 0.70972, val_loss: 1.39368, val_acc: 0.60558\n",
      "epoch: 94, train_loss: 1.28048, train_acc: 0.7109, val_loss: 1.38888, val_acc: 0.59798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 95, train_loss: 1.28021, train_acc: 0.71084, val_loss: 1.39724, val_acc: 0.60668\n",
      "epoch: 96, train_loss: 1.28202, train_acc: 0.71252, val_loss: 1.39004, val_acc: 0.60323\n",
      "epoch: 97, train_loss: 1.28036, train_acc: 0.7102, val_loss: 1.3909, val_acc: 0.60323\n",
      "epoch: 98, train_loss: 1.28, train_acc: 0.71244, val_loss: 1.39331, val_acc: 0.60654\n",
      "epoch: 99, train_loss: 1.28092, train_acc: 0.70988, val_loss: 1.38866, val_acc: 0.6104\n",
      "epoch: 100, train_loss: 1.28114, train_acc: 0.71162, val_loss: 1.39258, val_acc: 0.60489\n",
      "complete...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, X_val, y_val, 2e-3, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGdLfzx84AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb555ba7c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.draw_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60594594594594609"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(X_test, y_test, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
